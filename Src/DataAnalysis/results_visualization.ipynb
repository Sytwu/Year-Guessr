{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "U know... just importing required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the csv path of a dataset\n",
    "Please cheak the csv sample form from 'Dataset/csv/wiki_dataset.csv' to avoid unexpected results OuOb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../../Results/CLIP_Zeroshot_ALL.csv'\n",
    "# path = '../../Results/YearCLIPv3.csv'\n",
    "# path = '../../Results/NumCLIPv1.csv'\n",
    "\n",
    "# path = '../../Results/4o_mini.csv'\n",
    "# path = '../../Results/Gemini1.5.csv'\n",
    "# path = '../../Results/Claude3.csv'\n",
    "# path = '../../Results/Grok2.csv'\n",
    "\n",
    "# path = '../../Results/VLMs/cogvlm2-llama3-chat-19B.csv'\n",
    "# path = '../../Results/VLMs/gemma-3-4b-it.csv'\n",
    "# path = '../../Results/VLMs/gemma-3-12b-it.csv'\n",
    "# path = '../../Results/VLMs/glm-4v-9b.csv'\n",
    "# path = '../../Results/VLMs/InternVL2-2B.csv'\n",
    "# path = '../../Results/VLMs/InternVL2-4B.csv'\n",
    "# path = '../../Results/VLMs/InternVL2-8B.csv'\n",
    "# path = '../../Results/VLMs/InternVL2-26B.csv'\n",
    "# path = '../../Results/VLMs/InternVL3-2B.csv'\n",
    "# path = '../../Results/VLMs/InternVL3-8B.csv'\n",
    "# path = '../../Results/VLMs/InternVL3-9B.csv'\n",
    "# path = '../../Results/VLMs/llava-v16-vicuna-7b-hf.csv'\n",
    "# path = '../../Results/VLMs/llava-v16-vicuna-13b-hf.csv'\n",
    "# path = '../../Results/VLMs/llava15-7B-hf.csv'\n",
    "# path = '../../Results/VLMs/llava15-13B-hf.csv'\n",
    "# path = '../../Results/VLMs/MiniCPM-V-2_6.csv'\n",
    "# path = '../../Results/VLMs/Qwen25VL-3B.csv'\n",
    "# path = '../../Results/VLMs/Qwen25VL-7B.csv'\n",
    "path = '../../Results/VLMs/Qwen25VL-32B.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the comparison\n",
    "Scatter the points (prediction , ground-truth) and draw a ideal red line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = df['pred'].to_numpy()\n",
    "gt = df['year'].to_numpy()\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(pred, gt, alpha=0.5)\n",
    "plt.plot(pred, pred, color='red')\n",
    "\n",
    "plt.xlim(800, 2200)\n",
    "plt.ylim(800, 2200)\n",
    "\n",
    "plt.title(f\"Prediction Error of Year Built, MAE = {np.mean(np.abs(pred-gt)):.2f}\")\n",
    "plt.xlabel('Prediction (Year Built)')\n",
    "plt.ylabel('Ground Truth (Year Built)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show the difference\n",
    "Plot a histogram to visualize the difference between prediction and ground-truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = df['diff'].to_numpy()\n",
    "\n",
    "plt.hist(errors, bins=50, color='blue', edgecolor='black')\n",
    "\n",
    "plt.title(f\"Histogram of Prediction Error of Year Built, MAE = {np.mean(np.abs(errors)):.2f}\")\n",
    "plt.xlabel('Prediction Error (Years)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = df['diff'].to_numpy()\n",
    "\n",
    "plt.hist(errors, bins=200, color='blue')\n",
    "\n",
    "plt.title(f\"Histogram of Prediction Error of Year Built, MAE = {np.mean(np.abs(errors)):.2f}\")\n",
    "plt.xlabel('Prediction Error (Years)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.xlim(-200, 200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = df['diff'].to_numpy()\n",
    "\n",
    "plt.hist(errors, bins=range(-200,200+1))\n",
    "\n",
    "plt.title(f\"Histogram of Prediction Error of Year Built, MAE = {np.mean(np.abs(errors)):.2f}\")\n",
    "plt.xlabel('Prediction Error (Years)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Classification Accuracy\n",
    "Predefined bins = [0, 1150, 1400, 1600, 1800, 1900, 1950, float('inf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1150, 1400, 1600, 1800, 1900, 1950, float('inf')]\n",
    "labels = list(range(7))\n",
    "\n",
    "df['year_class'] = pd.cut(df['year'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "df['pred_class'] = pd.cut(df['pred'], bins=bins, labels=labels, right=True, include_lowest=True)\n",
    "\n",
    "correct_predictions = (df['year_class'] == df['pred_class']).sum()\n",
    "total_predictions = len(df)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'CLS Accuracyï¼š{accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOP100 worst prediction\n",
    "Display top100 prediction that miss the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_worst_pred(top_100_df):\n",
    "    img_folder = '../../Dataset/images/'\n",
    "\n",
    "    fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    fig.suptitle(\"TOP 100 Year Prediction Errors\\n\", fontsize=20)\n",
    "\n",
    "    for idx, (index, row) in enumerate(top_100_df.iterrows()):\n",
    "        ax = axes[idx // 10, idx % 10]\n",
    "        img = mpimg.imread(img_folder + row['name'])\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(f\"Year: {row['year']}, Pred: {row['pred']}\", fontsize=8)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_df = df.copy()\n",
    "abs_df['diff'] = abs_df['diff'].abs()\n",
    "\n",
    "show_worst_pred(abs_df.nlargest(100, 'diff'))\n",
    "# show_worst_pred(df.nsmallest(100, 'diff'))\n",
    "# show_worst_pred(df.nlargest(100, 'diff'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interval Performance\n",
    "Compute the accuracy within different intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_errors = np.array(abs_df['diff'].to_numpy())\n",
    "interval = [5,20,50,100]\n",
    "\n",
    "for i in interval:\n",
    "    print(f\"[{i}] {np.sum(abs_errors <= i) / len(abs_errors) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popularity (Views) and Prediction\n",
    "It indicates whether the predictions are susceptible to the building's popularity.\n",
    "\n",
    "We assume that the more famous a building is, the higher the number of views on the website will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('../../Dataset/csv/test.csv')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_df['Views'], abs_df['diff'], alpha=0.5)\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.title('Relation between Views and Prediction Error')\n",
    "plt.xlabel('Views (Times)')\n",
    "plt.ylabel('Prediction Error (Years)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the accuracy within 5 years in each interval of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 1e2, 1e3, 1e4, 1e5, np.inf]\n",
    "labels = ['<1e2', '1e2-1e3', '1e3-1e4', '1e4-1e5', '>1e5']\n",
    "\n",
    "test_df['Interval'] = pd.cut(test_df['Views'], bins=bins, labels=labels, right=False)\n",
    "result = abs_df[abs_df['diff'] <= 5].groupby(test_df['Interval'],observed=True).size()\n",
    "result /= test_df.groupby('Interval',observed=True).size()\n",
    "result = result.apply(lambda x: f'{x * 100:.2f}%')\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"MAE = {np.mean(np.abs(pred-gt)):.2f}\\n\")\n",
    "print(f\"CLS Acc = {accuracy:.2%}\\n\")\n",
    "\n",
    "for i in interval:\n",
    "    print(f\"[{i}] {np.sum(abs_errors <= i) / len(abs_errors) * 100:.2f}%\")\n",
    "\n",
    "print()\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
